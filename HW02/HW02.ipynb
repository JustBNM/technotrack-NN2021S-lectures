{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ №2 - обучение модели трехслойного перцептрона методом градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1)\n",
    "В качестве теоретического задания в этом ДЗ предлагается провести вывод функции ошибки для задачи регрессии в предположении, что целевая переменная подчиняется распределению Лапласа. Также предлагается воспользоваться байесовским выводом и в том же предположении относительно распределения целевой переменной вывести форму функции потерь с условием лапласовского априорного распределения параметров модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)\n",
    "В этом ДЗ предлагается реализовать модель трехслойного перцептрона и обучение этой модели методом градиентного спуска.\n",
    "\n",
    "На этот раз предлагается работать с реальными данными. Данные представляют из себя набор рукописных цифр. Это изображения размером 28х28. Каждому изображению поставлен в соответствие класс - арабская цифра. Задача модели - определить цифру, соответствующую произвольному изображению из тестового набора данных.\n",
    "\n",
    "Так же, как и в ДЗ №1, предлагается реализовать функцию потерь и саму модель перцептрона в манере, схожей с построением модулей фреймворка pytorch.\n",
    "\n",
    "В решении ожидается наличие следующих ключевых составляющих:<br />\n",
    "\n",
    "- (текст) формулировка задачи\n",
    "- (текст) формулировка признакового описания объектов\n",
    "- (текст, формулы) формулировка модели многослойного перцептрона\n",
    "- (текст, формулы) формулировка функции ошибки\n",
    "- (текст, формулы) формулировка меры качества модели\n",
    "- (текст, код и диаграммы) исследование исходных данных: распределение признаков и другие действия, дающие понимание о характере исходных данных\n",
    "- (текст, код, диаграммы) фильтрация признаков (при необходимости), порождение признаков (при необходимости)\n",
    "- (формулы, код, результаты, коммментарии) обучение модели методом градиентного спуска\n",
    "- (код, результаты, комментарии) оценка качества модели на валидационной выборке\n",
    "\n",
    "#### Код решения:\n",
    "(можно использовать предлагаемые шаблоны)\n",
    "- формулировка модели трехслойного перцептрона. Имеется в виду только один скрытый слой;\n",
    "- формулировка функции ошибки;\n",
    "- формулировка метрики (метрик);\n",
    "- формулировка цикла оптимизации параметров.\n",
    "\n",
    "#### Визуализация в решении:\n",
    "- распределение признаков;\n",
    "- распределение целевой переменной;\n",
    "- отдельные экземпляры выборки в виде изображений;\n",
    "- эволюция функции ошибки по ходу обучения;\n",
    "- эволюция метрики(метрик) по ходу обучения\n",
    "\n",
    "#### Выводы\n",
    "- вывод о достаточности или избыточности данных для оценки параметров модели\n",
    "- вывод о соотношении выразительности модели и ее обобщающей способности (наблюдаются ли явления переобучения или недообучения).\n",
    "\n",
    "Примечание:<br />\n",
    "Реализация перцептрона и других составляющих исследования может быть написана только с использованием библиотеки Numpy или scipy. Решения с использованием библиотек автоматического вычисления градиентов не засчитываются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исходные данные\n",
    "\n",
    "Исходные данные можно скачать [по этой ссылке](https://www.dropbox.com/s/y6ar7i7mb6fvoed/mnist.npz). Набор данных MNIST поставляется в различных вариантах. В варианте, доступном по приведенной ссылке, чтение исходных данных может быть выполнено следующим образом:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "mnist = np.load('mnist.npz')\n",
    "x_train = mnist['x_train']\n",
    "y_train = mnist['y_train']\n",
    "x_test = mnist['x_test']\n",
    "y_test = mnist['y_test']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многослойный перцептрон\n",
    "\n",
    "Напомним, мы задаем мнолослойный перцептрон как сложную функцию, в которой используются линейные операции и поточечные нелинейные преобразования. Если входные данные (признаковое описание объекта) заданы вектором $x$, то функция перцептрона с одним скрытым слоем выглядит следующим образом:\n",
    "$$\n",
    "F(x) = \\Psi\\left(\\phi\\left( {x}\\cdot\\theta_1 + b_1 \\right)\\cdot\\theta_2 + b_2\\right),\n",
    "$$\n",
    "где $x$ имеется в виду без дополнительного единичного признака; $\\phi$ - функция активации скрытого слоя; $\\Psi$ - функция активации выходного слоя перцептрона.\n",
    "\n",
    "Напомним также, что в задаче жесткой многоклассовой классификации на $K$ классов допустим вариант формулировки модели, такой что:\n",
    "- количество признаков целевой переменной совпадает с количеством классов $K$;\n",
    "- в качестве функции активации $\\Psi$ может использоваться `softmax`:\n",
    "$$\n",
    "\\Psi(h_i) = \\frac{e^{h_i}}{\\sum_{j=1}^{K}{e^{h_j}}}\n",
    "$$\n",
    "- в качестве функции потерь может использоваться перекрестная энтропия в многоклассовом варианте (приведено в записи для одного объекта):\n",
    "$$\n",
    "{\\mathscr{L}}\\left(\\hat{y},y\\right) = -\\sum_{j=1}^{K}{y_j*ln\\left(\\hat{y}_j\\right)},\n",
    "$$\n",
    "где $\\hat{y}=F(x)$\n",
    "\n",
    "В своем решении вы никак не ограничены в выборе функций активации $\\phi$ или $\\Psi$. Однако есть некоторые устоявшиеся практики применения функций `ReLU, sigmoid, tanh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Особенности реализации функции `softmax`\n",
    "Несложно заметить, что как в числителе, так и в знаменателе функции `softmax` стоит экспонента некоторого числа. При этом следует понимать, что разрядность чисел с плавающей точкой `float32`, `float64` и даже `float128` не бесконечны. Свойства экспоненты таковы, что, например, для переполнения разрядности чисел `float64` (максимум  $\\sim1.78*10^{308}$) достаточно показателя, превышающего 710, что совсем немного. Поэтому в случае практической реализации функции `softmax` имеет смысл предусмотреть случаи, когда аргументы экспоненты велики или, наоборот, слишком малы.\n",
    "\n",
    "В этом ДЗ кроме прочих заданий вам нужно реализовать вычислительно стабильную версию `softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Особенности вычисления градиента функции потерь\n",
    "\n",
    "В этом домашнем задании, также как и в ДЗ№1 предлагается реализовывать функцию потерь и отдельные вычислительные блоки перцептрона наследующими `Differentiable` для общности восприятия этих модулей как дифференцируемых по своим аргументам. По желанию можно вычислить градиент функции потерь по параметрам модели вручную (и далее реализовать его в коде), однако предпочитаемым способом будет реализация градиента каждого из вычислительных блоков по аргументу в методе `backward()` и использование этого результата в обобщенном виде, без упрощения. Этот вариант вычисления градиента функции потерь по параметрам модели называется \"backpropagation\" (\"метод обратного распространения ошибки\" или \"метод обратной волны\" у разных авторов).\n",
    "\n",
    "Нелишним будет напомнить, что в некоторых случаях для вычисления компоненты градиента необходимо хранить значения, полученные на этапе вычисления функции $F(x)$. В вашем решении это может быть устроено по-разному. Но для тех, кто хочет придерживаться предложенного шаблона, введен атрибут `state` класса `Differentiable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачей является классификация рукописных цифр на изображениях из датасета MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax применяется для нормировки вероятности к единице"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = np.load('./mnist.npz')\n",
    "x_train = mnist['x_train']\n",
    "y_train = mnist['y_train']\n",
    "# Обратите внимание на то, что целевая переменная в виде целых чисел от 0 до 9, в то время как в формулах,\n",
    "# приведенных выше, подразумевается one-hot кодирование целевой переменной\n",
    "x_test = mnist['x_test']\n",
    "y_test = mnist['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f47ce2dd940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "выпрямление матрицы в вектор и нормировка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(list(map(lambda x: x.reshape((784))/255, x_train)))\n",
    "x_test = list(map(lambda x: x.reshape((784))/255, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = []\n",
    "for i in range(x_train.shape[-1]):\n",
    "    counter = 0\n",
    "    for j in range(x_train.shape[0]):\n",
    "        if x_train[j][i] == 0:\n",
    "            counter+=1\n",
    "    frequency.append(counter/x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter=0\n",
    "index_to_delete = []\n",
    "for i in range(len(frequency)):\n",
    "    if frequency[i]>=0.995:\n",
    "        counter+=1\n",
    "        index_to_delete.append(i)\n",
    "index_to_delete.reverse()\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_reseve = [i for i in range(len(frequency))]\n",
    "for i in index_to_delete:\n",
    "    index_to_reseve.pop(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:, index_to_reseve]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим столбец единиц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_model = PolynomialFeatures(degree=1, include_bias=True)\n",
    "\n",
    "x_train = poly_model.fit_transform(x_train)\n",
    "x_test = poly_model.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot encoding можно было бы импортировать из sklearn, но сделаем сами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(index=None):\n",
    "    zero_list = [0]*10\n",
    "    zero_list[index] = 1\n",
    "    return zero_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(list(map(lambda x: one_hot(x), y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Differentiable:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        pass\n",
    "    \n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self, *args):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве функции потерь используем перекрестную энтропию   \n",
    "$$\n",
    "{\\mathscr{L}}\\left(\\hat{y},y\\right) = -\\sum_{j=1}^{K}{y_j*ln\\left(\\hat{y}_j\\right)},\n",
    "$$\n",
    "где $\\hat{y}=F(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(loss, self).__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_train):\n",
    "        # Этот метод реализует вычисление значения функции потерь\n",
    "        # Подсказка: метод должен возвращать единственный скаляр - значение функции потерь\n",
    "        self.state = y_train  \n",
    "        self.cache = y_pred\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        loss_value = sum(-sum(self.state * np.log(y_pred)))/self.state.shape[0]\n",
    "        \n",
    "        ###################\n",
    "        #accuracy_counter:#\n",
    "        ###################\n",
    "        y_pred = np.array(y_pred)\n",
    "        counter = 0\n",
    "        for i in range(len(y_pred)):\n",
    "            if np.where(y_pred[i]==max(y_pred[i]))[0][0] == np.where(y_train[i]==max(y_train[i]))[0][0]:\n",
    "                counter+=1\n",
    "            else: \n",
    "                print('y_train', y_train[i])\n",
    "                print('y_pred', y_pred[i])\n",
    "        accuracy = counter/len(y_pred)\n",
    "        print('--------------------------')\n",
    "    \n",
    "                \n",
    "        return loss_value, accuracy\n",
    "    \n",
    "    \n",
    "    def backward(self, out_shape):\n",
    "        # Этот метод реализует вычисление градиента функции потерь по аргументу y_pred\n",
    "        # Подсказка: метод должен возвращать вектор градиента функции потерь\n",
    "        #           размерностью, совпадающей с размерностью аргумента y_pred\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        y_pred = self.cache\n",
    "        partial_grad = (y_pred - self.state).T \n",
    "        \n",
    "        return partial_grad.reshape(1, out_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, x_train):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(x_train)\n",
    "    \n",
    "    def backward(self, upstream_grad, flag, out_shape):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        ### YOUR CODE HERE\n",
    "        in_shape = upstream_grad.shape[-1]\n",
    "        \n",
    "        if flag == 'theta':\n",
    "            local_grad = []\n",
    "            for i in range(self.y_pred.shape[0]):\n",
    "                for j in range(self.y_pred.shape[1]):\n",
    "                    cell_grad=[]\n",
    "                    for local_j in range(self.theta.shape[1]):\n",
    "                        if local_j == j:\n",
    "                            cell_grad.append(self.cache_x_train[i])\n",
    "                        else:\n",
    "                            cell_grad.append(np.zeros(len(self.cache_x_train[i])))\n",
    "                    cell_grad = np.array(cell_grad).T\n",
    "                    local_grad.append(cell_grad)\n",
    "                    \n",
    "            local_grad = np.array(local_grad)\n",
    "#             print('local_grad \\n',local_grad)\n",
    "#             print('cache_x_train \\n', self.cache_x_train)\n",
    "            #local_grad = local_grad.reshape(in_shape, out_shape, order='F')\n",
    "            local_grad = local_grad.reshape(in_shape, out_shape)\n",
    "#             print('----------------------')\n",
    "#             print('local_grad \\n',local_grad) \n",
    "                    \n",
    "            partial_grad = upstream_grad @ local_grad\n",
    "            \n",
    "        if flag == 'x_train':\n",
    "            \n",
    "            local_grad = []\n",
    "            for i in range(self.y_pred.shape[0]):\n",
    "                for j in range(self.y_pred.shape[1]):\n",
    "                    cell_grad=[]\n",
    "                    for local_i in range(self.cache_x_train.shape[0]):\n",
    "                        if local_i == i:\n",
    "                            cell_grad.append(np.array(self.theta[:,j]))\n",
    "                        else:\n",
    "                            cell_grad.append(np.zeros(len(self.theta[:,j])))\n",
    "                    \n",
    "                    local_grad.append(cell_grad)\n",
    "            \n",
    "            local_grad = np.array(local_grad) \n",
    "            local_grad = local_grad.reshape(in_shape, out_shape)        \n",
    "            partial_grad = upstream_grad @ local_grad\n",
    "            \n",
    "        return partial_grad\n",
    "\n",
    "    \n",
    "    def forward(self, x_train):\n",
    "        # этот метод предназначен для применения модели к данным\n",
    "        assert x_train.ndim == 2, \"X should be 2-dimensional: (N of objects, n of features)\"\n",
    "        \n",
    "#         x_train = poly_model.fit_transform(x_train)\n",
    "        \n",
    "        if (self.theta is None):\n",
    "            # Если вектор параметров еще не инициализирован, его следует инициализировать\n",
    "            # Подсказка: длина вектора параметров может быть получена из размера матрицы X\n",
    "\n",
    "            ### YOUR CODE HERE\n",
    "            self.theta = np.random.randn(x_train.shape[-1]*10).reshape(x_train.shape[-1],10) \n",
    "        \n",
    "        \n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        y_pred = x_train@self.theta \n",
    "        \n",
    "        self.y_pred = y_pred\n",
    "        self.cache_x_train = x_train\n",
    "        self.cache_theta = self.theta\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Activation, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, x_train):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(x_train)\n",
    "    \n",
    "    def backward(self, upstream_grad):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        local_grad = []\n",
    "        for i in range(self.value_sign.shape[0]):\n",
    "            for j in range(self.value_sign.shape[1]):\n",
    "                cell_grad = np.zeros(self.value_sign.shape)\n",
    "                cell_grad[i][j] = np.maximum(self.value_sign[i][j], 0)\n",
    "                local_grad.append(cell_grad)\n",
    "        local_grad = np.array(local_grad)\n",
    "            \n",
    "        local_grad = local_grad.reshape(upstream_grad.shape[-1], upstream_grad.shape[-1])\n",
    "                \n",
    "        partial_grad =  upstream_grad @ local_grad\n",
    "        \n",
    "        return partial_grad\n",
    "    \n",
    "    def forward(self, x_train):\n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        self.value_sign = np.sign(x_train)\n",
    "    \n",
    "        y_pred = np.maximum(x_train, 0)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Softmax, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(x_train)\n",
    "    \n",
    "    def backward(self):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        partial_grad = np.zeros((x_train.shape))\n",
    "        softmax_val = self.forward_val\n",
    "        \n",
    "        for i in range(partial_grad.shape[0]*len(softmax_val[0])):\n",
    "            for j in range(partial_grad.shape[1]):\n",
    "                if i != j:\n",
    "                    partial_grad[i:i+len(softmax_val[i])][j] = softmax_val[i] * softmax_val[j]\n",
    "                else:\n",
    "                    partial_grad[i:i+len(softmax_val[i])][j] = softmax_val[i]*(1 - softmax_val[i])\n",
    "        \n",
    "        return partial_grad \n",
    "    \n",
    "    def forward(self, y_in):\n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        y_pred_out = []\n",
    "        for line in y_in:\n",
    "            shift = line - np.max(line)\n",
    "            exps = np.exp(shift)\n",
    "            y_pred_line = exps / np.sum(exps)\n",
    "            \n",
    "            y_pred_out.append(y_pred_line)\n",
    "\n",
    "        self.forward_val = y_pred_out\n",
    "        \n",
    "        return y_pred_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        \n",
    "        self.Sm=Softmax()\n",
    "        self.activ=Activation()\n",
    "        self.lr_1=Linear()\n",
    "        self.lr_2=Linear()\n",
    "        self.parameters = None\n",
    "        \n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(x_train)\n",
    "    \n",
    "    def backward(self, x_train, upstream_grad, theta):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        self.theta = theta\n",
    "        \n",
    "        upstream_grad_1 = self.lr_2.backward(upstream_grad, 'x_train',  x_train.shape[0]*10)\n",
    "        upstream_grad_1 = self.activ.backward(upstream_grad_1).T\n",
    "        \n",
    "        grad_1 = self.lr_1.backward(upstream_grad_1.T, 'theta', self.theta.shape[0]*self.theta.shape[1])\n",
    "\n",
    "#         partial_grad_2 = self.lr_2.backward(upstream_grad, 'theta')\n",
    "        partial_grad_2 = None\n",
    "        \n",
    "        return grad_1, partial_grad_2\n",
    "    \n",
    "    def forward(self, x_train):\n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "        \n",
    "        y_pred = self.Sm.forward(self.lr_2.forward(self.activ.forward(self.lr_1.forward(x_train))))\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(x_train, y_train, model, loss_fn, epochs=100):\n",
    "    #np.random.seed(1)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    pbar = tqdm(total=epochs)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # В этом цикле следует реализовать итеративную процедуру оптимизации параметров модели model,\n",
    "        #        руководствуясь функцией потерь loss_fn\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        #batch selection\n",
    "        batch_size = 1\n",
    "        alpha = 1e-2\n",
    "        \n",
    "        sh = list(range(x_train.shape[0])) \n",
    "        #np.random.shuffle(sh)\n",
    "        sh = sh[:batch_size]\n",
    "        \n",
    "        x_batch_train = x_train[sh,:]\n",
    "        y_batch_train = y_train[sh,:]\n",
    "\n",
    "            \n",
    "        loss_value, accuracy = loss_fn.forward(model.forward(x_batch_train), y_batch_train)\n",
    "        grad_1, grad_2 = model.backward(x_batch_train, loss_fn.backward(x_batch_train.shape[0]*10), model.lr_1.theta)\n",
    "\n",
    "        grad_1 = grad_1.reshape(model.lr_1.theta.shape)\n",
    "        model.lr_1.theta = model.lr_1.theta - alpha*grad_1\n",
    "        #model.lr_2.theta = model.lr_2.theta - alpha*grad_2.T\n",
    "\n",
    "        \n",
    "        loss_history.append(loss_value)\n",
    "        accuracy_history.append(accuracy)\n",
    "        pbar.update(1)\n",
    "        #print(loss_value)\n",
    "        pbar.set_postfix({'loss': loss_value, 'accuracy': accuracy})\n",
    "        print('----------------')\n",
    "    pbar.close()\n",
    "    return loss_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:00<00:00, 46.96it/s, loss=46.9, accuracy=0]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:00<00:00, 48.06it/s, loss=25.7, accuracy=0]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:00<00:00, 48.23it/s, loss=9.31, accuracy=0]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 4/10 [00:00<00:00, 48.08it/s, loss=7.64, accuracy=0]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [00:00<00:00, 49.00it/s, loss=7.64, accuracy=0]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 5/10 [00:00<00:00, 49.00it/s, loss=6.01, accuracy=0]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:00<00:00, 49.00it/s, loss=4.39, accuracy=0]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 7/10 [00:00<00:00, 49.00it/s, loss=2.83, accuracy=0]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 8/10 [00:00<00:00, 49.00it/s, loss=1.5, accuracy=0] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [00:00<00:00, 49.00it/s, loss=0.696, accuracy=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train [0 0 0 0 0 1 0 0 0 0]\n",
      "y_pred [3.37870848e-22 1.00000000e+00 1.38258719e-16 6.40647907e-23\n",
      " 5.35136053e-19 4.17093342e-21 2.74809165e-20 5.49117208e-19\n",
      " 2.68966300e-13 1.85304161e-10]\n",
      "--------------------------\n",
      "----------------\n",
      "y_train [0 0 0 0 0 1 0 0 0 0]\n",
      "y_pred [3.65035568e-17 9.99999893e-01 4.25519026e-13 1.64611525e-17\n",
      " 1.09557328e-16 6.95491667e-12 1.87923896e-16 2.17878051e-15\n",
      " 1.01701967e-07 5.74197087e-09]\n",
      "--------------------------\n",
      "----------------\n",
      "y_train [0 0 0 0 0 1 0 0 0 0]\n",
      "y_pred [1.76382856e-12 9.93790273e-01 2.63469765e-10 7.87345197e-13\n",
      " 1.59037481e-14 9.08089046e-05 5.92456472e-13 4.17632663e-12\n",
      " 6.11884957e-03 6.83860862e-08]\n",
      "--------------------------\n",
      "----------------\n",
      "y_train [0 0 0 0 0 1 0 0 0 0]\n",
      "y_pred [4.77830983e-11 9.89776743e-01 6.05240171e-10 4.71082410e-12\n",
      " 1.23074779e-13 4.80973908e-04 4.85050747e-12 4.98036837e-12\n",
      " 9.74208825e-03 1.93711822e-07]\n",
      "--------------------------\n",
      "----------------\n",
      "y_train [0 0 0 0 0 1 0 0 0 0]\n",
      "y_pred [1.23892407e-09 9.82748957e-01 1.37719845e-09 2.79028541e-11\n",
      " 9.22573546e-13 2.46515296e-03 3.86032678e-11 5.77787070e-12\n",
      " 1.47853370e-02 5.50704349e-07]\n",
      "--------------------------\n",
      "----------------\n",
      "y_train [0 0 0 0 0 1 0 0 0 0]\n",
      "y_pred [3.14143142e-08 9.65401502e-01 3.08662777e-09 1.62267169e-10\n",
      " 6.78289618e-12 1.24329511e-02 3.01389943e-10 6.61172219e-12\n",
      " 2.21639701e-02 1.54196940e-06]\n",
      "--------------------------\n",
      "----------------\n",
      "y_train [0 0 0 0 0 1 0 0 0 0]\n",
      "y_pred [7.33730948e-07 9.09348453e-01 6.55128116e-09 8.83467706e-10\n",
      " 4.65399007e-11 5.89615690e-02 2.19569917e-09 7.21428494e-12\n",
      " 3.16851525e-02 4.08198051e-06]\n",
      "--------------------------\n",
      "----------------\n",
      "y_train [0 0 0 0 0 1 0 0 0 0]\n",
      "y_pred [1.25762208e-05 7.39130137e-01 1.14802761e-08 3.78862452e-09\n",
      " 2.48497570e-10 2.22587986e-01 1.24211929e-08 6.70741935e-12\n",
      " 3.82604363e-02 8.83649597e-06]\n",
      "--------------------------\n",
      "----------------\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 44.08it/s, loss=0.696, accuracy=1]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 44.08it/s, loss=0.375, accuracy=1]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "--------------------------\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "obj_fn = loss()\n",
    "model = Perceptron()\n",
    "loss_history, accuracy_history = train_loop(x_train, y_train, model, obj_fn, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.title('loss function train')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss value')\n",
    "plt.show()\n",
    "plt.plot(accuracy_history)\n",
    "plt.title('accuracy train')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "b = np.array([[[1],[1],[1]], [[2],[2],[2]], [[3],[3],[3]]])\n",
    "\n",
    "# b.reshape(9,1)\n",
    "b.shape\n",
    "b.reshape(9,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(np.zeros((2, 2)), dtype=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[list([5, 1]), list([5, 1])],\n",
       "       [list([5, 1]), list([5, 1])]], dtype=object)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(a.shape[0]):\n",
    "    for j in range(a.shape[1]):\n",
    "        a[i][j] = [5,1]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
