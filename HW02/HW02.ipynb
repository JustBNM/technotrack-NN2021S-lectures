{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ №2 - обучение модели трехслойного перцептрона методом градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1)\n",
    "В качестве теоретического задания в этом ДЗ предлагается провести вывод функции ошибки для задачи регрессии в предположении, что целевая переменная подчиняется распределению Лапласа. Также предлагается воспользоваться байесовским выводом и в том же предположении относительно распределения целевой переменной вывести форму функции потерь с условием лапласовского априорного распределения параметров модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)\n",
    "В этом ДЗ предлагается реализовать модель трехслойного перцептрона и обучение этой модели методом градиентного спуска.\n",
    "\n",
    "На этот раз предлагается работать с реальными данными. Данные представляют из себя набор рукописных цифр. Это изображения размером 28х28. Каждому изображению поставлен в соответствие класс - арабская цифра. Задача модели - определить цифру, соответствующую произвольному изображению из тестового набора данных.\n",
    "\n",
    "Так же, как и в ДЗ №1, предлагается реализовать функцию потерь и саму модель перцептрона в манере, схожей с построением модулей фреймворка pytorch.\n",
    "\n",
    "В решении ожидается наличие следующих ключевых составляющих:<br />\n",
    "\n",
    "- (текст) формулировка задачи\n",
    "- (текст) формулировка признакового описания объектов\n",
    "- (текст, формулы) формулировка модели многослойного перцептрона\n",
    "- (текст, формулы) формулировка функции ошибки\n",
    "- (текст, формулы) формулировка меры качества модели\n",
    "- (текст, код и диаграммы) исследование исходных данных: распределение признаков и другие действия, дающие понимание о характере исходных данных\n",
    "- (текст, код, диаграммы) фильтрация признаков (при необходимости), порождение признаков (при необходимости)\n",
    "- (формулы, код, результаты, коммментарии) обучение модели методом градиентного спуска\n",
    "- (код, результаты, комментарии) оценка качества модели на валидационной выборке\n",
    "\n",
    "#### Код решения:\n",
    "(можно использовать предлагаемые шаблоны)\n",
    "- формулировка модели трехслойного перцептрона. Имеется в виду только один скрытый слой;\n",
    "- формулировка функции ошибки;\n",
    "- формулировка метрики (метрик);\n",
    "- формулировка цикла оптимизации параметров.\n",
    "\n",
    "#### Визуализация в решении:\n",
    "- распределение признаков;\n",
    "- распределение целевой переменной;\n",
    "- отдельные экземпляры выборки в виде изображений;\n",
    "- эволюция функции ошибки по ходу обучения;\n",
    "- эволюция метрики(метрик) по ходу обучения\n",
    "\n",
    "#### Выводы\n",
    "- вывод о достаточности или избыточности данных для оценки параметров модели\n",
    "- вывод о соотношении выразительности модели и ее обобщающей способности (наблюдаются ли явления переобучения или недообучения).\n",
    "\n",
    "Примечание:<br />\n",
    "Реализация перцептрона и других составляющих исследования может быть написана только с использованием библиотеки Numpy или scipy. Решения с использованием библиотек автоматического вычисления градиентов не засчитываются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исходные данные\n",
    "\n",
    "Исходные данные можно скачать [по этой ссылке](https://www.dropbox.com/s/y6ar7i7mb6fvoed/mnist.npz). Набор данных MNIST поставляется в различных вариантах. В варианте, доступном по приведенной ссылке, чтение исходных данных может быть выполнено следующим образом:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "mnist = np.load('mnist.npz')\n",
    "x_train = mnist['x_train']\n",
    "y_train = mnist['y_train']\n",
    "x_test = mnist['x_test']\n",
    "y_test = mnist['y_test']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многослойный перцептрон\n",
    "\n",
    "Напомним, мы задаем мнолослойный перцептрон как сложную функцию, в которой используются линейные операции и поточечные нелинейные преобразования. Если входные данные (признаковое описание объекта) заданы вектором $x$, то функция перцептрона с одним скрытым слоем выглядит следующим образом:\n",
    "$$\n",
    "F(x) = \\Psi\\left(\\phi\\left( {x}\\cdot\\theta_1 + b_1 \\right)\\cdot\\theta_2 + b_2\\right),\n",
    "$$\n",
    "где $x$ имеется в виду без дополнительного единичного признака; $\\phi$ - функция активации скрытого слоя; $\\Psi$ - функция активации выходного слоя перцептрона.\n",
    "\n",
    "Напомним также, что в задаче жесткой многоклассовой классификации на $K$ классов допустим вариант формулировки модели, такой что:\n",
    "- количество признаков целевой переменной совпадает с количеством классов $K$;\n",
    "- в качестве функции активации $\\Psi$ может использоваться `softmax`:\n",
    "$$\n",
    "\\Psi(h_i) = \\frac{e^{h_i}}{\\sum_{j=1}^{K}{e^{h_j}}}\n",
    "$$\n",
    "- в качестве функции потерь может использоваться перекрестная энтропия в многоклассовом варианте (приведено в записи для одного объекта):\n",
    "$$\n",
    "{\\mathscr{L}}\\left(\\hat{y},y\\right) = -\\sum_{j=1}^{K}{y_j*ln\\left(\\hat{y}_j\\right)},\n",
    "$$\n",
    "где $\\hat{y}=F(x)$\n",
    "\n",
    "В своем решении вы никак не ограничены в выборе функций активации $\\phi$ или $\\Psi$. Однако есть некоторые устоявшиеся практики применения функций `ReLU, sigmoid, tanh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Особенности реализации функции `softmax`\n",
    "Несложно заметить, что как в числителе, так и в знаменателе функции `softmax` стоит экспонента некоторого числа. При этом следует понимать, что разрядность чисел с плавающей точкой `float32`, `float64` и даже `float128` не бесконечны. Свойства экспоненты таковы, что, например, для переполнения разрядности чисел `float64` (максимум  $\\sim1.78*10^{308}$) достаточно показателя, превышающего 710, что совсем немного. Поэтому в случае практической реализации функции `softmax` имеет смысл предусмотреть случаи, когда аргументы экспоненты велики или, наоборот, слишком малы.\n",
    "\n",
    "В этом ДЗ кроме прочих заданий вам нужно реализовать вычислительно стабильную версию `softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Особенности вычисления градиента функции потерь\n",
    "\n",
    "В этом домашнем задании, также как и в ДЗ№1 предлагается реализовывать функцию потерь и отдельные вычислительные блоки перцептрона наследующими `Differentiable` для общности восприятия этих модулей как дифференцируемых по своим аргументам. По желанию можно вычислить градиент функции потерь по параметрам модели вручную (и далее реализовать его в коде), однако предпочитаемым способом будет реализация градиента каждого из вычислительных блоков по аргументу в методе `backward()` и использование этого результата в обобщенном виде, без упрощения. Этот вариант вычисления градиента функции потерь по параметрам модели называется \"backpropagation\" (\"метод обратного распространения ошибки\" или \"метод обратной волны\" у разных авторов).\n",
    "\n",
    "Нелишним будет напомнить, что в некоторых случаях для вычисления компоненты градиента необходимо хранить значения, полученные на этапе вычисления функции $F(x)$. В вашем решении это может быть устроено по-разному. Но для тех, кто хочет придерживаться предложенного шаблона, введен атрибут `state` класса `Differentiable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачей является классификация рукописных цифр на изображениях из датасета MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax применяется для нормировки вероятности к единице"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = np.load('./mnist.npz')\n",
    "x_train = mnist['x_train']\n",
    "y_train = mnist['y_train']\n",
    "# Обратите внимание на то, что целевая переменная в виде целых чисел от 0 до 9, в то время как в формулах,\n",
    "# приведенных выше, подразумевается one-hot кодирование целевой переменной\n",
    "x_test = mnist['x_test']\n",
    "y_test = mnist['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efbce9c6ac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "выпрямление матрицы в вектор и нормировка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(list(map(lambda x: x.reshape((784))/255, x_train)))\n",
    "x_test = list(map(lambda x: x.reshape((784))/255, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(x_train.shape[-1]):\n",
    "#     for j in range(x_train.shape[0]):\n",
    "#         if x_train[j][i] == 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим столбец единиц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_model = PolynomialFeatures(degree=1, include_bias=True)\n",
    "\n",
    "x_train = poly_model.fit_transform(x_train)\n",
    "x_test = poly_model.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot encoding можно было бы импортировать из sklearn, но сделаем сами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(index=None):\n",
    "    zero_list = [0]*10\n",
    "    zero_list[index] = 1\n",
    "    return zero_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(list(map(lambda x: one_hot(x), y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Differentiable:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        pass\n",
    "    \n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self, *args):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве функции потерь используем перекрестную энтропию   \n",
    "$$\n",
    "{\\mathscr{L}}\\left(\\hat{y},y\\right) = -\\sum_{j=1}^{K}{y_j*ln\\left(\\hat{y}_j\\right)},\n",
    "$$\n",
    "где $\\hat{y}=F(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(loss, self).__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_train):\n",
    "        # Этот метод реализует вычисление значения функции потерь\n",
    "        # Подсказка: метод должен возвращать единственный скаляр - значение функции потерь\n",
    "        self.state = y_train\n",
    "        \n",
    "        self.cache = y_pred\n",
    "        \n",
    "        print('y_train', self.state)\n",
    "        print('y_pred', y_pred)\n",
    "        ### YOUR CODE HERE\n",
    "        loss_value = sum(-sum(self.state * np.log(y_pred)))/self.state.shape[0]\n",
    "                \n",
    "        return loss_value\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "        # Этот метод реализует вычисление градиента функции потерь по аргументу y_pred\n",
    "        # Подсказка: метод должен возвращать вектор градиента функции потерь\n",
    "        #           размерностью, совпадающей с размерностью аргумента y_pred\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        y_pred = self.cache\n",
    "        partial_grad = (y_pred - self.state).T \n",
    "        print('(y_pred - self.state)', (y_pred - self.state))\n",
    "        \n",
    "        return partial_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, x_train):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(x_train)\n",
    "    \n",
    "    def backward(self, upstream_grad):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "#         print('upstream_grad_l1', upstream_grad.shape)\n",
    "#         print('x_train_l1', self.cache.shape)\n",
    "        partial_grad = upstream_grad.T @ self.cache\n",
    "        #partial_grad = x_train \n",
    "        \n",
    "#         print('partial_grad', partial_grad[0][0])\n",
    "        \n",
    "        return partial_grad\n",
    "\n",
    "    \n",
    "    def forward(self, x_train):\n",
    "        # этот метод предназначен для применения модели к данным\n",
    "        assert x_train.ndim == 2, \"X should be 2-dimensional: (N of objects, n of features)\"\n",
    "        \n",
    "#         x_train = poly_model.fit_transform(x_train)\n",
    "        \n",
    "        if (self.theta is None):\n",
    "            # Если вектор параметров еще не инициализирован, его следует инициализировать\n",
    "            # Подсказка: длина вектора параметров может быть получена из размера матрицы X\n",
    "\n",
    "            ### YOUR CODE HERE\n",
    "            self.theta = np.random.randn(x_train.shape[-1]*10).reshape(x_train.shape[-1],10)\n",
    "        \n",
    "        \n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        y_pred = x_train@self.theta\n",
    "        \n",
    "        self.cache = x_train\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_hiden(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Linear_hiden, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, x_train):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(x_train)\n",
    "    \n",
    "    def backward(self, upstream_grad):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        print('upstream_grad_l2', upstream_grad.shape)\n",
    "        print('x_train_l2', self.cache.shape)\n",
    "        partial_grad = upstream_grad.T @ self.cache.T\n",
    "\n",
    "        \n",
    "        print('partial_grad', partial_grad[0][0])\n",
    "        \n",
    "        return partial_grad\n",
    "\n",
    "    \n",
    "    def forward(self, x_train):\n",
    "        # этот метод предназначен для применения модели к данным\n",
    "        assert x_train.ndim == 2, \"X should be 2-dimensional: (N of objects, n of features)\"\n",
    "        \n",
    "#         x_train = poly_model.fit_transform(x_train)\n",
    "        \n",
    "        if (self.theta is None):\n",
    "            # Если вектор параметров еще не инициализирован, его следует инициализировать\n",
    "            # Подсказка: длина вектора параметров может быть получена из размера матрицы X\n",
    "\n",
    "            ### YOUR CODE HERE\n",
    "            self.theta = np.random.randn(x_train.shape[-1]*10).reshape(x_train.shape[-1],10)\n",
    "        \n",
    "        \n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        y_pred = x_train@self.theta\n",
    "        \n",
    "        self.cache = self.theta\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Activation, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, x_train):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(x_train)\n",
    "    \n",
    "    def backward(self, upstream_grad):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        #print('np.maximum(self.value_sign, 0).T', np.maximum(self.value_sign, 0).T)\n",
    "        print('activation', upstream_grad.shape, np.maximum(self.value_sign, 0).shape)\n",
    "        \n",
    "        partial_grad =  upstream_grad * np.maximum(self.value_sign, 0)\n",
    "        #partial_grad = upstream_grad\n",
    "        \n",
    "#         print('partial_activ ', partial_grad.shape, '= upstream ', upstream_grad.shape, \n",
    "#                            ' * sign ', np.maximum(self.value_sign, 0).T.shape)\n",
    "        \n",
    "        return partial_grad\n",
    "    \n",
    "    def forward(self, x_train):\n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        #x_train = poly_model.fit_transform(x_train)\n",
    "        print('x_train_activ', x_train)\n",
    "        self.value_sign = np.sign(x_train)\n",
    "    \n",
    "        y_pred = np.maximum(x_train, 0)\n",
    "        #y_pred = x_train\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Softmax, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(x_train)\n",
    "    \n",
    "    def backward(self):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        partial_grad = np.zeros((x_train.shape))\n",
    "        #softmax_val = Softmax.forward(x_train)\n",
    "        softmax_val = self.forward_val\n",
    "        \n",
    "        for i in range(partial_grad.shape[0]*len(softmax_val[0])):\n",
    "            for j in range(partial_grad.shape[1]):\n",
    "                if i != j:\n",
    "                    partial_grad[i:i+len(softmax_val[i])][j] = softmax_val[i] * softmax_val[j]\n",
    "                else:\n",
    "                    partial_grad[i:i+len(softmax_val[i])][j] = softmax_val[i]*(1 - softmax_val[i])\n",
    "        \n",
    "        return partial_grad \n",
    "    \n",
    "    def forward(self, y_pred):\n",
    "        # args = [x]\n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        shifty = y_pred - np.max(y_pred)\n",
    "        exps = np.exp(shifty)\n",
    "        y_pred = exps / np.sum(exps)\n",
    "        \n",
    "        self.forward_val = y_pred\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        \n",
    "        self.Sm=Softmax()\n",
    "        self.activ=Activation()\n",
    "        self.lr_1=Linear()\n",
    "        self.lr_2=Linear_hiden()\n",
    "        self.parameters = None\n",
    "        \n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(x_train)\n",
    "    \n",
    "    def backward(self, x_train, upstream_grad):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        #partial_grad = self.Sm.backward(self.activ.backward(self.lr.backward(x_train)))\n",
    "#         partial_grad = self.activ.backward(self.lr.backward(x_train))\n",
    "        #print('percept upstream_grad', upstream_grad.shape)\n",
    "        #print('percept local_grad ', self.lr.backward(x_train, self.activ.backward()).shape)\n",
    "        partial_grad = self.lr_1.backward(self.activ.backward(self.lr_2.backward(upstream_grad)))\n",
    "        #partial_grad = self.lr.backward(x_train)\n",
    "        \n",
    "        return partial_grad\n",
    "    \n",
    "    def forward(self, x_train, theta):\n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "        \n",
    "        #y_pred = self.Sm.forward(self.lr.forward(x_train))\n",
    "        y_pred = self.Sm.forward(self.lr_2.forward(self.activ.forward(self.lr_1.forward(x_train))))\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(x_train, y_train, model, loss_fn, epochs=100):\n",
    "    np.random.seed(1)\n",
    "    loss_history = []\n",
    "    pbar = tqdm(total=epochs)\n",
    "    for epoch in range(epochs):\n",
    "        # В этом цикле следует реализовать итеративную процедуру оптимизации параметров модели model,\n",
    "        #        руководствуясь функцией потерь loss_fn\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        #batch selection\n",
    "        batch_size = 1\n",
    "        alpha = 1e-2\n",
    "        \n",
    "        concat = np.concatenate((x_train, y_train), axis=1)\n",
    "        #np.random.shuffle(concat)\n",
    "        concat = concat[:batch_size]\n",
    "        x_batch_train = concat[:,0:785]\n",
    "        y_batch_train = concat[:,785:795]\n",
    "#         x_batch_train=np.array([x_train[1]])\n",
    "#         y_batch_train=np.array([y_train[1]])\n",
    "            \n",
    "        loss_value = loss_fn.forward(model.forward(x_batch_train, model.lr_1.theta), y_batch_train)\n",
    "        #grad = loss_fn.backward(model.forward(x_train), model.backward(x_train))\n",
    "        grad = model.backward(x_batch_train, loss_fn.backward())\n",
    "        print('grad', (alpha*grad.T[0][:10]))\n",
    "        print('theta', model.lr_1.theta[0][:10])\n",
    "\n",
    "        model.lr_1.theta = model.lr_1.theta - alpha*grad.T\n",
    "        #print(model.theta)\n",
    "#         print(model.theta[0][0])\n",
    "#         print((alpha*grad.T)[0][0])\n",
    "        \n",
    "        loss_history.append(loss_value)\n",
    "        pbar.update(1)\n",
    "        #print(loss_value)\n",
    "        pbar.set_postfix({'loss': loss_value})\n",
    "        print('----------------')\n",
    "    pbar.close()\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|          | 1/100 [00:00<00:36,  2.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|          | 1/100 [00:00<00:36,  2.70it/s, loss=48.8]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    0.15281851  12.6143932    5.87761329  11.11346296  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[1.56438048e-13 1.34346554e-21 7.29665046e-12 1.02159574e-22\n",
      "  1.59800178e-06 6.33688572e-22 1.72134705e-15 1.42355383e-09\n",
      "  5.85562901e-13 9.99998401e-01]]\n",
      "(y_pred - self.state) [[ 1.56438048e-13  1.34346554e-21  7.29665046e-12  1.02159574e-22\n",
      "   1.59800178e-06 -1.00000000e+00  1.72134705e-15  1.42355383e-09\n",
      "   5.85562901e-13  9.99998401e-01]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.18605625533789766\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00902062\n",
      "  0.01259891  0.01082502  0.02401864  0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.3015387\n",
      "  1.74481176 -0.7612069   0.3190391  -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 2/100 [00:00<00:37,  2.60it/s, loss=48.8]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▏         | 2/100 [00:00<00:37,  2.60it/s, loss=40.2]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    0.98586858  11.45088966   4.87792706   8.89535229  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[3.81875557e-12 5.25445977e-19 2.38471288e-09 1.32499065e-19\n",
      "  8.57165140e-06 3.53656215e-18 6.82637980e-13 3.42358796e-07\n",
      "  3.31209549e-11 9.99991084e-01]]\n",
      "(y_pred - self.state) [[ 3.81875557e-12  5.25445977e-19  2.38471288e-09  1.32499065e-19\n",
      "   8.57165140e-06 -1.00000000e+00  6.82637980e-13  3.42358796e-07\n",
      "   3.31209549e-11  9.99991084e-01]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.186053303938286\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00902068\n",
      "  0.01259892  0.01082487  0.02401862  0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.29251808\n",
      "  1.73221286 -0.77203193  0.29502046 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 3/100 [00:01<00:38,  2.53it/s, loss=40.2]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|▎         | 3/100 [00:01<00:38,  2.53it/s, loss=31.6]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    1.8189244   10.28738501   3.87825487   6.6772437   -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[9.32038488e-11 2.05482755e-16 7.79281465e-07 1.71823879e-16\n",
      "  4.59714064e-05 1.97347577e-14 2.70677852e-10 8.23229053e-05\n",
      "  1.87313506e-09 9.99870924e-01]]\n",
      "(y_pred - self.state) [[ 9.32038488e-11  2.05482755e-16  7.79281465e-07  1.71823879e-16\n",
      "   4.59714064e-05 -1.00000000e+00  2.70677852e-10  8.23229053e-05\n",
      "   1.87313506e-09  9.99870924e-01]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.18608324801457665\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00902066\n",
      "  0.01259991  0.01082161  0.02401724  0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.2834974\n",
      "  1.71961394 -0.7828568   0.27100184 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 4/100 [00:01<00:41,  2.30it/s, loss=31.6]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|▍         | 4/100 [00:01<00:41,  2.30it/s, loss=22.9]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    2.65197803   9.12378874   2.87888358   4.45926189  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[2.22853103e-09 7.87669468e-14 2.49525921e-04 2.18301977e-13\n",
      "  2.41556319e-04 1.07906568e-10 1.05152502e-07 1.93830107e-02\n",
      "  1.03785602e-07 9.80125696e-01]]\n",
      "(y_pred - self.state) [[ 2.22853103e-09  7.87669468e-14  2.49525921e-04  2.18301977e-13\n",
      "   2.41556319e-04 -1.00000000e+00  1.05152502e-07  1.93830107e-02\n",
      "   1.03785602e-07  9.80125696e-01]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.19699835686761458\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.0089363\n",
      "  0.01282379  0.010229    0.02371352  0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.27447674\n",
      "  1.70701403 -0.79367841  0.2469846  -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 5/100 [00:02<00:43,  2.20it/s, loss=22.9]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 5/100 [00:02<00:43,  2.20it/s, loss=16]  \u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    3.47724152   7.93951731   1.93423956   2.26932855  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[1.01770943e-08 6.36686951e-12 1.54322389e-02 5.31695024e-11\n",
      "  2.44539566e-04 1.14328733e-07 7.76167414e-06 7.76996358e-01\n",
      "  1.10169911e-06 2.07317875e-01]]\n",
      "(y_pred - self.state) [[ 1.01770943e-08  6.36686951e-12  1.54322389e-02  5.31695024e-11\n",
      "   2.44539566e-04 -9.99999886e-01  7.76167414e-06  7.76996358e-01\n",
      "   1.10169911e-06  2.07317875e-01]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.6273694469722443\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00554868\n",
      "  0.02162914 -0.01290368  0.01166955  0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.26554044\n",
      "  1.69419023 -0.80390742  0.22327107 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 6/100 [00:02<00:42,  2.22it/s, loss=16]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|▌         | 6/100 [00:02<00:42,  2.22it/s, loss=10.4]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    3.98965976   5.9420757    3.12588855   1.19165096  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[6.88386961e-09 3.55012732e-09 2.04523377e-01 2.14010433e-09\n",
      "  5.06650908e-05 2.96496569e-05 6.65093636e-05 4.90315928e-02\n",
      "  1.92418246e-06 7.46296270e-01]]\n",
      "(y_pred - self.state) [[ 6.88386961e-09  3.55012732e-09  2.04523377e-01  2.14010433e-09\n",
      "   5.06650908e-05 -9.99970350e-01  6.65093636e-05  4.90315928e-02\n",
      "   1.92418246e-06  7.46296270e-01]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.15590515058948992\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00856586\n",
      "  0.01415541  0.00832163  0.01794772  0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.25999176\n",
      "  1.67256109 -0.79100374  0.21160153 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 7/100 [00:03<00:41,  2.23it/s, loss=10.4]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|▋         | 7/100 [00:03<00:41,  2.23it/s, loss=6.1] \u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    4.78071295   4.63482988   2.35738968  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[2.84219815e-08 8.56695270e-08 6.85156496e-01 6.78861743e-08\n",
      "  2.40507930e-05 2.25273885e-03 9.56892549e-04 1.78520303e-01\n",
      "  1.31897836e-05 1.33076147e-01]]\n",
      "(y_pred - self.state) [[ 2.84219815e-08  8.56695270e-08  6.85156496e-01  6.78861743e-08\n",
      "   2.40507930e-05 -9.97747261e-01  9.56892549e-04  1.78520303e-01\n",
      "   1.31897836e-05  1.33076147e-01]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.09376568791346754\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00740827\n",
      "  0.01794468  0.00196993  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.2514259\n",
      "  1.65840568 -0.79932537  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 8/100 [00:03<00:43,  2.13it/s, loss=6.1]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 8/100 [00:03<00:43,  2.13it/s, loss=2.69]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    5.46486357   2.97764653   2.17546734  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[5.11125173e-07 6.92815184e-06 5.98055204e-01 1.47078745e-06\n",
      "  2.53030731e-05 6.75805015e-02 1.20968537e-02 1.02715658e-01\n",
      "  2.81854949e-04 2.19235714e-01]]\n",
      "(y_pred - self.state) [[ 5.11125173e-07  6.92815184e-06  5.98055204e-01  1.47078745e-06\n",
      "   2.53030731e-05 -9.32419498e-01  1.20968537e-02  1.02715658e-01\n",
      "   2.81854949e-04  2.19235714e-01]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.07628699623756287\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00718455\n",
      "  0.01573794  0.00378033  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.24401763\n",
      "  1.640461   -0.8012953   0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 9/100 [00:04<00:43,  2.10it/s, loss=2.69]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|▉         | 9/100 [00:04<00:43,  2.10it/s, loss=0.532]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    6.12835376   1.52425438   1.82635596  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[3.74625250e-06 1.24959678e-04 1.98886110e-01 1.03167075e-05\n",
      "  1.24268360e-05 5.87675870e-01 5.66445498e-02 3.92454793e-02\n",
      "  2.33522015e-03 1.15061321e-01]]\n",
      "(y_pred - self.state) [[ 3.74625250e-06  1.24959678e-04  1.98886110e-01  1.03167075e-05\n",
      "   1.24268360e-05 -4.12324130e-01  5.66445498e-02  3.92454793e-02\n",
      "   2.33522015e-03  1.15061321e-01]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.11104594422389373\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00294378\n",
      "  0.00615458  0.00114896  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.23683308\n",
      "  1.62472306 -0.80507563  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 10%|█         | 10/100 [00:04<00:41,  2.19it/s, loss=0.532]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 10/100 [00:04<00:41,  2.19it/s, loss=0.229]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    6.40021035   0.95588164   1.72024997  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[4.39525986e-06 2.23810647e-04 7.56895941e-02 1.22763757e-05\n",
      "  5.18986226e-06 7.94936392e-01 5.84750193e-02 1.46513295e-02\n",
      "  2.94466745e-03 5.30573251e-02]]\n",
      "(y_pred - self.state) [[ 4.39525986e-06  2.23810647e-04  7.56895941e-02  1.22763757e-05\n",
      "   5.18986226e-06 -2.05063608e-01  5.84750193e-02  1.46513295e-02\n",
      "   2.94466745e-03  5.30573251e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.09611439614133213\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00131205\n",
      "  0.00263923  0.00021024  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.2338893\n",
      "  1.61856848 -0.80622459  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 11%|█         | 11/100 [00:04<00:40,  2.22it/s, loss=0.229]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|█         | 11/100 [00:04<00:40,  2.22it/s, loss=0.163]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    6.5213774    0.71215037   1.70083438  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[4.17716470e-06 2.70719024e-04 4.75209686e-02 1.20639856e-05\n",
      "  3.24222007e-06 8.49672222e-01 5.44170365e-02 8.54893714e-03\n",
      "  2.93155833e-03 3.66190748e-02]]\n",
      "(y_pred - self.state) [[ 4.17716470e-06  2.70719024e-04  4.75209686e-02  1.20639856e-05\n",
      "   3.24222007e-06 -1.50327778e-01  5.44170365e-02  8.54893714e-03\n",
      "   2.93155833e-03  3.66190748e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.08563553227425992\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -9.04132996e-04  1.77235435e-03  1.96071747e-05\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.23257725\n",
      "  1.61592925 -0.80643483  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 12/100 [00:05<00:40,  2.19it/s, loss=0.163]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▏        | 12/100 [00:05<00:40,  2.19it/s, loss=0.131]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    6.60487368   0.54847423   1.69902366  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[3.88979845e-06 3.04148555e-04 3.44994100e-02 1.16252309e-05\n",
      "  2.30074070e-06 8.77412693e-01 5.07046276e-02 5.74569238e-03\n",
      "  2.83557905e-03 2.84800333e-02]]\n",
      "(y_pred - self.state) [[ 3.88979845e-06  3.04148555e-04  3.44994100e-02  1.16252309e-05\n",
      "   2.30074070e-06 -1.22587307e-01  5.07046276e-02  5.74569238e-03\n",
      "   2.83557905e-03  2.84800333e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.07809661795066392\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -7.05638712e-04  1.35572857e-03 -5.82630247e-05\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.23167312\n",
      "  1.6141569  -0.80645443  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 13/100 [00:05<00:39,  2.21it/s, loss=0.131]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█▎        | 13/100 [00:05<00:39,  2.21it/s, loss=0.111]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    6.6700391    0.42327329   1.70440423  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[3.61131204e-06 3.31071625e-04 2.69499037e-02 1.11581016e-05\n",
      "  1.74558782e-06 8.94777131e-01 4.75174948e-02 4.16061421e-03\n",
      "  2.72135984e-03 2.35259095e-02]]\n",
      "(y_pred - self.state) [[ 3.61131204e-06  3.31071625e-04  2.69499037e-02  1.11581016e-05\n",
      "   1.74558782e-06 -1.05222869e-01  4.75174948e-02  4.16061421e-03\n",
      "   2.72135984e-03  2.35259095e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.07223373890945792\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -5.85695946e-04  1.10679893e-03 -9.73567773e-05\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.23096748\n",
      "  1.61280117 -0.80639617  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 14/100 [00:06<00:37,  2.27it/s, loss=0.111]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█▍        | 14/100 [00:06<00:37,  2.27it/s, loss=0.0977]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    6.72412787   0.3210609    1.71339508  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[3.35630362e-06 3.54087386e-04 2.20177769e-02 1.07072253e-05\n",
      "  1.38148158e-06 9.06915763e-01 4.47713593e-02 3.15881166e-03\n",
      "  2.60591590e-03 2.01608407e-02]]\n",
      "(y_pred - self.state) [[ 3.35630362e-06  3.54087386e-04  2.20177769e-02  1.07072253e-05\n",
      "   1.38148158e-06 -9.30842368e-02  4.47713593e-02  3.15881166e-03\n",
      "   2.60591590e-03  2.01608407e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.06745234767464094\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00050445\n",
      "  0.0009399  -0.00011888  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.23038178\n",
      "  1.61169437 -0.80629881  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 15%|█▌        | 15/100 [00:06<00:36,  2.33it/s, loss=0.0977]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▌        | 15/100 [00:06<00:36,  2.33it/s, loss=0.0877]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    6.77071336   0.23426165   1.7243732   -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[3.12631538e-06 3.74462387e-04 1.85447073e-02 1.02840851e-05\n",
      "  1.12599692e-06 9.16002768e-01 4.23779640e-02 2.47921237e-03\n",
      "  2.49482795e-03 1.77115213e-02]]\n",
      "(y_pred - self.state) [[ 3.12631538e-06  3.74462387e-04  1.85447073e-02  1.02840851e-05\n",
      "   1.12599692e-06 -8.39972318e-02  4.23779640e-02  2.47921237e-03\n",
      "   2.49482795e-03  1.77115213e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.06342857055312585\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00044533\n",
      "  0.00081958 -0.00013117  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.22987734\n",
      "  1.61075447 -0.80617994  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 16%|█▌        | 16/100 [00:07<00:35,  2.37it/s, loss=0.0877]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|█▌        | 16/100 [00:07<00:35,  2.37it/s, loss=0.08]  \u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    6.81183902   0.15857345   1.73648666  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[2.91957458e-06 3.92913342e-04 1.59690421e-02 9.89055930e-06\n",
      "  9.38090199e-07 9.23130042e-01 4.02681795e-02 1.99479550e-03\n",
      "  2.38995012e-03 1.58413290e-02]]\n",
      "(y_pred - self.state) [[ 2.91957458e-06  3.92913342e-04  1.59690421e-02  9.89055930e-06\n",
      "   9.38090199e-07 -7.68699578e-02  4.02681795e-02  1.99479550e-03\n",
      "   2.38995012e-03  1.58413290e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.059966287666347616\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00040013\n",
      "  0.00072839 -0.00013816  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.22943201\n",
      "  1.60993489 -0.80604877  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 17/100 [00:07<00:35,  2.31it/s, loss=0.08]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|█▋        | 17/100 [00:07<00:35,  2.31it/s, loss=0.0737]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    6.84879114   0.09130664   1.74924562  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[2.73355817e-06 4.09887218e-04 1.39848031e-02 9.52541664e-06\n",
      "  7.94980384e-07 9.28912058e-01 3.83897710e-02 1.63655570e-03\n",
      "  2.29172407e-03 1.43621467e-02]]\n",
      "(y_pred - self.state) [[ 2.73355817e-06  4.09887218e-04  1.39848031e-02  9.52541664e-06\n",
      "   7.94980384e-07 -7.10879417e-02  3.83897710e-02  1.63655570e-03\n",
      "   2.29172407e-03  1.43621467e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.05693741696494009\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00036432\n",
      "  0.00065669 -0.00014191  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.22903188\n",
      "  1.6092065  -0.80591061  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 18/100 [00:07<00:35,  2.33it/s, loss=0.0737]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 18/100 [00:07<00:35,  2.33it/s, loss=0.0686]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    6.88243592   0.03066197   1.76235078  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[2.56575985e-06 4.25682606e-04 1.24108893e-02 9.18645964e-06\n",
      "  6.83005105e-07 9.33723847e-01 3.67030305e-02 1.36394010e-03\n",
      "  2.20001846e-03 1.31601568e-02]]\n",
      "(y_pred - self.state) [[ 2.56575985e-06  4.25682606e-04  1.24108893e-02  9.18645964e-06\n",
      "   6.83005105e-07 -6.62761531e-02  3.67030305e-02  1.36394010e-03\n",
      "   2.20001846e-03  1.31601568e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.054253485036699356\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00033515\n",
      "  0.00059869 -0.00014359  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.22866756\n",
      "  1.60854981 -0.8057687   0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 19/100 [00:08<00:34,  2.35it/s, loss=0.0686]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|█▉        | 19/100 [00:08<00:34,  2.35it/s, loss=0.0656]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    6.91338671  -0.02462639   1.77561135  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[2.40794382e-06 4.27824057e-04 1.16060969e-02 8.79182605e-06\n",
      "  6.13090947e-07 9.36518632e-01 3.55669578e-02 1.22047673e-03\n",
      "  2.11993854e-03 1.25282609e-02]]\n",
      "(y_pred - self.state) [[ 2.40794382e-06  4.27824057e-04  1.16060969e-02  8.79182605e-06\n",
      "   6.13090947e-07 -6.34813679e-02  3.55669578e-02  1.22047673e-03\n",
      "   2.11993854e-03  1.25282609e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.052471338588159874\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.0003191\n",
      "  0.         -0.00014184  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.22833241\n",
      "  1.60795112 -0.80562511  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 20%|██        | 20/100 [00:08<00:34,  2.34it/s, loss=0.0656]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 20/100 [00:08<00:34,  2.34it/s, loss=0.0645]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    6.94285525  -0.02462639   1.78871011  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[2.25779238e-06 4.15263353e-04 1.14408560e-02 8.33817948e-06\n",
      "  5.74609379e-07 9.37564376e-01 3.49547273e-02 1.17491703e-03\n",
      "  2.05103916e-03 1.23876505e-02]]\n",
      "(y_pred - self.state) [[ 2.25779238e-06  4.15263353e-04  1.14408560e-02  8.33817948e-06\n",
      "   5.74609379e-07 -6.24356239e-02  3.49547273e-02  1.17491703e-03\n",
      "   2.05103916e-03  1.23876505e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.051529303521698176\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00031421\n",
      "  0.         -0.00013743  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.22801331\n",
      "  1.60795112 -0.80548327  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 21%|██        | 21/100 [00:09<00:33,  2.36it/s, loss=0.0645]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|██        | 21/100 [00:09<00:33,  2.36it/s, loss=0.0634]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    6.97187244  -0.02462639   1.80140192  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[2.11988654e-06 4.03218333e-04 1.12788220e-02 7.91558598e-06\n",
      "  5.39171576e-07 9.38574337e-01 3.43672244e-02 1.13215003e-03\n",
      "  1.98594632e-03 1.22477272e-02]]\n",
      "(y_pred - self.state) [[ 2.11988654e-06  4.03218333e-04  1.12788220e-02  7.91558598e-06\n",
      "   5.39171576e-07 -6.14256630e-02  3.43672244e-02  1.13215003e-03\n",
      "   1.98594632e-03  1.22477272e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.050626553441055565\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00030946\n",
      "  0.         -0.00013331  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.2276991\n",
      "  1.60795112 -0.80534584  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 22/100 [00:09<00:32,  2.39it/s, loss=0.0634]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▏       | 22/100 [00:09<00:32,  2.39it/s, loss=0.0624]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    7.00045076  -0.02462639   1.81371325  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[1.99298327e-06 3.91663906e-04 1.11200144e-02 7.52138478e-06\n",
      "  5.06486624e-07 9.39550489e-01 3.38027833e-02 1.09193256e-03\n",
      "  1.92435485e-03 1.21087412e-02]]\n",
      "(y_pred - self.state) [[ 1.99298327e-06  3.91663906e-04  1.11200144e-02  7.52138478e-06\n",
      "   5.06486624e-07 -6.04495110e-02  3.38027833e-02  1.09193256e-03\n",
      "   1.92435485e-03  1.21087412e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.049760370786502715\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00030484\n",
      "  0.         -0.00012945  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.22738964\n",
      "  1.60795112 -0.80521253  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 23/100 [00:10<00:32,  2.38it/s, loss=0.0624]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|██▎       | 23/100 [00:10<00:32,  2.38it/s, loss=0.0613]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    7.02860221  -0.02462639   1.82566819  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[1.87599098e-06 3.80576193e-04 1.09644357e-02 7.15319018e-06\n",
      "  4.76296272e-07 9.40494645e-01 3.32598941e-02 1.05404873e-03\n",
      "  1.86599165e-03 1.19709033e-02]]\n",
      "(y_pred - self.state) [[ 1.87599098e-06  3.80576193e-04  1.09644357e-02  7.15319018e-06\n",
      "   4.76296272e-07 -5.95053551e-02  3.32598941e-02  1.05404873e-03\n",
      "   1.86599165e-03  1.19709033e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.04892829949749221\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00030034\n",
      "  0.         -0.00012583  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.22708481\n",
      "  1.60795112 -0.80508307  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 24/100 [00:10<00:31,  2.41it/s, loss=0.0613]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██▍       | 24/100 [00:10<00:31,  2.41it/s, loss=0.0604]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    7.05633835  -0.02462639   1.83728869  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[1.76794802e-06 3.69932496e-04 1.08120746e-02 6.80885809e-06\n",
      "  4.48370718e-07 9.41408475e-01 3.27371849e-02 1.01830626e-03\n",
      "  1.81061153e-03 1.18343905e-02]]\n",
      "(y_pred - self.state) [[ 1.76794802e-06  3.69932496e-04  1.08120746e-02  6.80885809e-06\n",
      "   4.48370718e-07 -5.85915255e-02  3.27371849e-02  1.01830626e-03\n",
      "   1.81061153e-03  1.18343905e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.04812811348352708\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00029596\n",
      "  0.         -0.00012243  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.22678447\n",
      "  1.60795112 -0.80495724  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 25/100 [00:10<00:32,  2.28it/s, loss=0.0604]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 25/100 [00:10<00:32,  2.28it/s, loss=0.0594]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    7.08367034  -0.02462639   1.84859483  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[1.66800461e-06 3.59711264e-04 1.06629085e-02 6.48645729e-06\n",
      "  4.22504996e-07 9.42293520e-01 3.22334060e-02 9.84533364e-04\n",
      "  1.75799375e-03 1.16993500e-02]]\n",
      "(y_pred - self.state) [[ 1.66800461e-06  3.59711264e-04  1.06629085e-02  6.48645729e-06\n",
      "   4.22504996e-07 -5.77064799e-02  3.22334060e-02  9.84533364e-04\n",
      "   1.75799375e-03  1.16993500e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.04735778957899629\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.0002917\n",
      "  0.         -0.00011922  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.22648851\n",
      "  1.60795112 -0.80483481  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 26%|██▌       | 26/100 [00:11<00:32,  2.27it/s, loss=0.0594]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|██▌       | 26/100 [00:11<00:32,  2.27it/s, loss=0.0585]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    7.11060894  -0.02462639   1.859605    -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[1.57540770e-06 3.49892049e-04 1.05169052e-02 6.18424464e-06\n",
      "  3.98515894e-07 9.43151210e-01 3.17474160e-02 9.52576065e-04\n",
      "  1.70793903e-03 1.15659035e-02]]\n",
      "(y_pred - self.state) [[ 1.57540770e-06  3.49892049e-04  1.05169052e-02  6.18424464e-06\n",
      "   3.98515894e-07 -5.68487900e-02  3.17474160e-02  9.52576065e-04\n",
      "   1.70793903e-03  1.15659035e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.046615484256000476\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00028755\n",
      "  0.         -0.0001162   0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.2261968\n",
      "  1.60795112 -0.80471559  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 27/100 [00:11<00:31,  2.29it/s, loss=0.0585]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██▋       | 27/100 [00:11<00:31,  2.29it/s, loss=0.0576]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    7.13716451  -0.02462639   1.87033605  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[1.48948810e-06 3.40455465e-04 1.03740248e-02 5.90064359e-06\n",
      "  3.76239299e-07 9.43982870e-01 3.12781701e-02 9.22295983e-04\n",
      "  1.66026697e-03 1.14341502e-02]]\n",
      "(y_pred - self.state) [[ 1.48948810e-06  3.40455465e-04  1.03740248e-02  5.90064359e-06\n",
      "   3.76239299e-07 -5.60171300e-02  3.12781701e-02  9.22295983e-04\n",
      "   1.66026697e-03  1.14341502e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.045899513500071175\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00028352\n",
      "  0.         -0.00011335  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.22590925\n",
      "  1.60795112 -0.80459939  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 28/100 [00:12<00:30,  2.33it/s, loss=0.0576]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 28/100 [00:12<00:30,  2.33it/s, loss=0.0568]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    7.16334706  -0.02462639   1.88080348  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[1.40964960e-06 3.31383142e-04 1.02342213e-02 5.63422556e-06\n",
      "  3.55527912e-07 9.44789734e-01 3.08247100e-02 8.93568407e-04\n",
      "  1.61481385e-03 1.13041703e-02]]\n",
      "(y_pred - self.state) [[ 1.40964960e-06  3.31383142e-04  1.02342213e-02  5.63422556e-06\n",
      "   3.55527912e-07 -5.52102664e-02  3.08247100e-02  8.93568407e-04\n",
      "   1.61481385e-03  1.13041703e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.04520833535869969\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00027958\n",
      "  0.         -0.00011065  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.22562573\n",
      "  1.60795112 -0.80448604  0.19365381 -0.24937038]\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 29%|██▉       | 29/100 [00:12<00:30,  2.34it/s, loss=0.0568]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|██▉       | 29/100 [00:12<00:30,  2.34it/s, loss=0.056] \u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    7.18916624  -0.02462639   1.89102159  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[1.33535972e-06 3.22657675e-04 1.00974435e-02 5.38369361e-06\n",
      "  3.36249268e-07 9.45572951e-01 3.03861542e-02 8.66280652e-04\n",
      "  1.57143078e-03 1.11760271e-02]]\n",
      "(y_pred - self.state) [[ 1.33535972e-06  3.22657675e-04  1.00974435e-02  5.38369361e-06\n",
      "   3.36249268e-07 -5.44270493e-02  3.03861542e-02  8.66280652e-04\n",
      "   1.57143078e-03  1.11760271e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n",
      "partial_grad -0.044540534757241006\n",
      "activation (1, 10) (1, 10)\n",
      "grad [ 0.          0.          0.          0.          0.         -0.00027575\n",
      "  0.         -0.00010809  0.          0.        ]\n",
      "theta [ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.22534615\n",
      "  1.60795112 -0.8043754   0.19365381 -0.24937038]\n",
      "----------------\n",
      "x_train_activ [[-13.92850683  -8.0674499   -6.42433508  -5.96489216 -19.30512189\n",
      "    7.21463137  -0.02462639   1.90100352  -0.46581271  -8.88229746]]\n",
      "y_train [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "y_pred [[1.26614171e-06 3.14262583e-04 9.96363660e-03 5.14786831e-06\n",
      "  3.18284026e-07 9.46333595e-01 2.99616910e-02 8.40330646e-04\n",
      "  1.52998199e-03 1.10497694e-02]]\n",
      "(y_pred - self.state) [[ 1.26614171e-06  3.14262583e-04  9.96363660e-03  5.14786831e-06\n",
      "   3.18284026e-07 -5.36664046e-02  2.99616910e-02  8.40330646e-04\n",
      "   1.52998199e-03  1.10497694e-02]]\n",
      "upstream_grad_l2 (10, 1)\n",
      "x_train_l2 (10, 10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-2e096cf7fc5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mobj_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-af5960073b4c>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(x_train, y_train, model, loss_fn, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#grad = loss_fn.backward(model.forward(x_train), model.backward(x_train))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'theta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-52cadc806ca6>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x_train, upstream_grad)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#print('percept upstream_grad', upstream_grad.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print('percept local_grad ', self.lr.backward(x_train, self.activ.backward()).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mpartial_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupstream_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m#partial_grad = self.lr.backward(x_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-7e236c224b80>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, upstream_grad)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'upstream_grad_l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupstream_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x_train_l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpartial_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupstream_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obj_fn = loss()\n",
    "model = Perceptron()\n",
    "loss_history = train_loop(x_train, y_train, model, obj_fn, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ результатов"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
