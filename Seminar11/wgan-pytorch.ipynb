{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN-GP in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import torch.autograd as autograd\n",
    "from scipy import stats\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Tuple, List, Type, Dict, Any\n",
    "from os.path import join, isfile, isdir\n",
    "from queue import Empty, Queue\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "### Здесь есть ошибка! Какая? ####\n",
    "##################################\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs=128):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_inputs, 512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "\n",
    "\n",
    "    def forward(self, x_noise):\n",
    "        x = x_noise  # N x n_inputs\n",
    "        \n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))  # N x 512\n",
    "        x = F.relu(self.fc2(x))               # N x 1024\n",
    "        \n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "### Здесь есть ошибка (при условии, что это WGAN)! Какая? ####\n",
    "##############################################################\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "### Здесь есть ошибка! Какая? ####\n",
    "##################################\n",
    "\n",
    "gen = Generator(n_inputs=latent_dim)\n",
    "gen = gen.to(DEVICE)\n",
    "\n",
    "dsc = Discriminator()\n",
    "dsc = dsc.to(DEVICE)\n",
    "\n",
    "optimizer_gen = torch.optim.Adam(gen.parameters(), lr=1e-4)\n",
    "optimizer_dsc = torch.optim.SGD(dsc.parameters(), lr=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "###        Train loop         ####\n",
    "##################################\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(f'Starting epoch {epoch + 1} of {max_epochs}')\n",
    "\n",
    "    gen.train()\n",
    "    dsc.train()\n",
    "\n",
    "    with tqdm(total=STEPS_PER_EPOCH) as pbar:\n",
    "        for idx in range(STEPS_PER_EPOCH):\n",
    "            # Training discriminator\n",
    "            optimizer_dsc.zero_grad()\n",
    "\n",
    "            real_batch = next(train_ds).to(DEVICE)\n",
    "            noise = Variable(torch.tensor(np.random.normal(0, 1, (len(real_batch), latent_dim)), dtype=torch.float, device=DEVICE))\n",
    "            fake_batch = gen(noise)\n",
    "            \n",
    "            fake_batch = torch.mul(fake_batch, mask_batch)\n",
    "\n",
    "            real_scores = dsc(real_batch)\n",
    "            fake_scores = dsc(fake_batch)\n",
    "            gradient_penalty = compute_gradient_penalty(dsc, real_batch, fake_batch)\n",
    "\n",
    "            loss_dsc = -torch.mean(real_scores) + torch.mean(fake_scores) + lambda_gp * gradient_penalty\n",
    "\n",
    "            loss_dsc.backward()\n",
    "            optimizer_dsc.step()\n",
    "\n",
    "            # Training generator\n",
    "            optimizer_gen.zero_grad()\n",
    "            fake_batch = gen(noise)\n",
    "            fake_batch = torch.mul(fake_batch, mask_batch)\n",
    "\n",
    "            fake_scores = dsc(fake_batch)\n",
    "            loss_gen = -torch.mean(fake_scores)\n",
    "\n",
    "            loss_gen.backward()\n",
    "            optimizer_gen.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'step': idx + 1, 'loss_GEN': loss_gen.item(), 'loss_DSC': loss_dsc.item()})\n",
    "\n",
    "    # estimating WGAN-GP loss after an epoch\n",
    "    gen.eval()\n",
    "    dsc.eval()\n",
    "    val_loss_epoch = []\n",
    "\n",
    "    with tqdm(total=VAL_STEPS) as pbar:\n",
    "        for idx in range(VAL_STEPS):\n",
    "            real_batch = next(train_ds).to(DEVICE)\n",
    "            noise = Variable(torch.tensor(np.random.normal(0, 1, (len(real_batch), latent_dim)), dtype=torch.float, device=DEVICE))\n",
    "            fake_batch = gen(noise)\n",
    "            fake_batch = torch.mul(fake_batch, mask_batch)\n",
    "            val_loss_epoch.append((torch.mean(dsc(real_batch) - dsc(fake_batch))).detach().cpu())\n",
    "            pbar.update(1)\n",
    "\n",
    "    val_loss_epoch = np.sum(val_loss_epoch)/(VAL_STEPS*VAL_BATCH_SIZE)\n",
    "\n",
    "    lr_scheduler_gen.step()\n",
    "    lr_scheduler_dsc.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
